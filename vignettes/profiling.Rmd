---
title: "profiling"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{profiling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = FALSE,
  comment = "#>"
)
```

# Synopsis
* TCA is a great package. But it is running unexpectedly slowly. A project with
a moderately sized (400,000 CpGs * 1000 Samples * 6 Cell Types, vars.mle = FALSE)
couldn't converge after 12 hours and hit out of memory error with 120GB of RAM.
* This fork is created to profile the performance of TCA and see if there are 
bottlenecks we can resolve.  

# Changelog
* Replaced `data.frame()` calls and `lm()` calls in `./R/model_fit.R`
* Added `fastLM_ftest()` to `./R/utils.R` to calculate partial f-tests.  

```{r setup, eval = FALSE}
library(profvis) # Profiling tool
library(devtools)
library(tictoc)
near <- dplyr::near
load_all()
```

```{r setup, eval = FALSE}
# Simulate the data
set.seed(1234)
data <- test_data(20, 10000, 6, 1, 1, 0.01)
tca.mdl <- tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2, vars.mle = TRUE)
lapply(data, dim)

set.seed(1234)
# Run the data sequentially first.
# sequential <- profvis({
#   tca.mdl <- tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2)
# })
#
# saveRDS(sequential, "./assets/sequential_n20_m1e5.rds")
# use_build_ignore("./assets/sequential_n20_m1e5.rds")
# use_git_ignore("./assets/sequential_n20_m1e5.rds")
```

```{r setup, eval = FALSE}
sequential <- readRDS("./assets/sequential_n20_m1e5.rds")
sequential
```

* The sequential run took 7' to be done
* We see that in `model_fit.R/tca_fit.R`, r is spending 
  + High peak memory and decent amount of time to call `tca.fit_means_vars`. 
  This is the internal loop convergence of TCA. This is probably what 
  bottle necked the loop in parallel mode because the high memory overhead.  
  + High peak memory and a lot of time to call `lm()` and subsequently 
  `model.frame.default()`  
  + The `anova.lm()` call to compare the model with a null model is also taking
  a significant amount time. This should be unnecessary since the model it is
  comparing to is a null model, the F statistics is already calculated in the 
  `summary.lm()` object  
* Let's try to replace the `data.frame()` call with a straight `cbind()` call to 
create a X matrix. Then we can use `RcppEigen::fastLm()` to directly call 
the data matrix and the vector y.  
  + This is potentially a dangerous trade-off between speed and safety. But 
  since NA is already expected to be taken care of by user, we should be safe to 
  make this trade off. Other issues such as full rank matrix should still cause 
  errors. We can create a test case for this.  
  + Have to implement an `anova()` method for `RcppEigen::fastLm()`.  

# Replacing codes
* Warning, this might be unsafe

## fastLm
* Replace the `data.frame()` and `lm()` call with `RcppEigen::fastLm`
```{r, eval = F}
df <-
  data.frame(y = X_tilde[, j], cbind(
    W / t(repmat(W_norms[, j], k, 1)),
    if (p2 > 0) {
      C2 / t(repmat(W_norms[, j], p2, 1))
    } else {
      C2
    },
    if (p1 > 0) {
      C1_ / t(repmat(W_norms[, j], k * p1, 1))
    } else {
      C1_
    }
  ))
mdl1.fit <- lm(y ~ ., data = df)
mdl1.coef <- summary(mdl1.fit)$coefficients
mdl1.cov.names <- colnames(df)[colnames(df) != "y"]
deltas_gammas_hat_pvals <-
  sapply(mdl1.cov.names, function(x) {
    if (x %in% rownames(mdl1.coef)) {
      return(mdl1.coef[x, "Pr(>|t|)"])
    } else {
      return(NA)
    }
  })
```

```{r, eval = F}
# Remove call to data.frame() and lm()
mdl1.fit <- RcppEigen::fastLm(
  X = cbind(
    "(Intercept)" = 1.0, # <----------- Remember the intercept
    W / t(repmat(W_norms[, j], k, 1)),
    if (p2 > 0) {
      C2 / t(repmat(W_norms[, j], p2, 1))
    } else {
      C2
    },
    if (p1 > 0) {
      C1_ / t(repmat(W_norms[, j], k * p1, 1))
    } else {
      C1_
    }
  ),
  y = X_tilde[, j]
)
mdl1.coef <- summary(mdl1.fit)$coefficients
# First row is always intercept. Sacrifice some code readability here
# Sacrifice some code readability here by using -1 instead of
## `which(rownames(mdl1.coef) != "(Intercept)")`
deltas_gammas_hat_pvals <- mdl1.coef[-1, "Pr(>|t|)"]
```

Sample with change 1
```{r, eval = F}
# Before
C1_alt <- C1_ / t(repmat(W_norms[, j], k * p1, 1))
for (d in 1:p1) {
  C1_null <- C1_alt[, setdiff(1:(p1 * k), seq(d, k * p1, p1))]
  df <-
    data.frame(y = X_tilde[, j], cbind(W / t(repmat(W_norms[, j], k, 1)), if (p2 > 0) {
      C2 / t(repmat(W_norms[, j], p2, 1))
    } else {
      C2
    }, C1_null))
  mdl0.fit <- lm(y ~ ., data = df)
  anova.fit <- anova(mdl0.fit, mdl1.fit)
  gammas_hat_pvals.joint[d] <- anova.fit$`Pr(>F)`[2]
}
```

```{r, eval = F}
# After
for (d in 1:p1) {
  mdl0.fit <- RcppEigen::fastLm(
    X = cbind(
      "(Intercept)" = 1.0, # <----------- Remember the intercept
      W / t(repmat(W_norms[, j], k, 1)),
      if (p2 > 0) {
        C2 / t(repmat(W_norms[, j], p2, 1))
      } else {
        C2
      },
      #### Used to be `C1_null` and `C1_alt`. Removed assignment calls.
      (C1_ / t(repmat(W_norms[, j], k * p1, 1)))[, setdiff(1:(p1 * k), seq(d, k * p1, p1))]
    ),
    y = X_tilde[, j]
  )
  gammas_hat_pvals.joint[d] <- fastLM_ftest(mdl0.fit, mdl1.fit)$`Pr(>F)`
}
```

## repmat
* repmat is taking a decent chunk out of the time. `MESS::repmat` is about 50%
faster at the current dimension.  
```{r}
use_package("MESS")
```

* Replace a bunch of `t(repmat ...)` with `MESS::repmat` to get rid of a t()
call and use the more efficient `MESS::repmat()` call.  

* For `vars.mle = TRUE`, carefully reproduce the gradient calculation and replace
with `MESS::repmat()` as well as removing some repeated calculations.  
```{r, eval = FALSE}
# Before
return(list(
  "objective" = -0.5 * (const - sum(log(V)) - sum(U_j / V)),
  "gradient" = -(colSums(W_squared * repmat(sigmas, n, 1) * t(repmat(U_j, k, 1)) /
    repmat(V_squared, 1, k)) - colSums(W_squared * repmat(sigmas, n, 1) / repmat(V, 1, k)))
))
```

```{r}
# After
W_squared_sig <- W_squared * MESS::repmat(matrix(sigmas, nrow = 1), nrow = n, 1)
return(list(
  "objective" = -0.5 * (const - sum(log(V)) - sum(U_j / V)),
  "gradient" = -(
    colSums(W_squared_sig * MESS::repmat(matrix(U_j), ncol = k) /
      MESS::repmat(V_squared, 1, ncol = k)) -
      colSums(W_squared_sig / MESS::repmat(V, 1, ncol = k))
  )
))
```

## removing memory consuming `<-` calls
* Check for memory consuming `<-` calls

# Test
## Did results changed?
```{r}
n_test <- 2
data <- readRDS(test_path("fixtures", "exp1_simdata.rds"))
fit_1_2_1_results <- readRDS(test_path("fixtures", "exp1_simdata_fit_1_2_1.rds"))
fit_1_2_1_results[seq_len(n_test), ]
set.seed(1234)

fit_1_2_1_results$new_fit <- lapply(
  data$df,
  \(d) {
    tca(
      X = d$X,
      W = d$W,
      C1 = d$C1,
      C2 = d$C2
    )
  }
)

compare_fit <- function(o, n) {
  all(sapply(names(o), \(x) {
    all(near(o[[x]], n[[x]]))
  }))
}

fit_1_2_1_results$results <- purrr::map2_lgl(
  fit_1_2_1_results$fit_1_2_1,
  fit_1_2_1_results$new_fit,
  compare_fit
)

plan(sequential)
# waldo::compare(fit_1_2_1_results$deltas_hat_pvals, tca.mdl$deltas_hat_pvals)
```

## Change 1 and 2
1 and 2 has to be tested concurrently
```{r}
data <- test_data(20, 10000, 6, 1, 1, 0.01)
tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2)
df <- readRDS("./assets/change_1.rds")
X <- cbind("(Intercept)" = 1, df[, which(names(df) != "y")])
y <- df$y

mdl1.fit <- lm(y ~ ., data = df)
mdl1.coef <- summary(mdl1.fit)$coefficients
mdl1.cov.names <- colnames(df)[colnames(df) != "y"]
deltas_gammas_hat_pvals <-
  sapply(mdl1.cov.names, function(x) {
    if (x %in% rownames(mdl1.coef)) {
      return(mdl1.coef[x, "Pr(>|t|)"])
    } else {
      return(NA)
    }
  })

deltas_gammas_hat_pvals

mdl1.fit.1 <- RcppEigen::fastLm(
  X = X,
  y = y
)

mdl1.coef.1 <- summary(mdl1.fit.1)$coefficients
deltas_gammas_hat_pvals.1 <- mdl1.coef.1[-1, "Pr(>|t|)"]
stopifnot(all(dplyr::near(deltas_gammas_hat_pvals, deltas_gammas_hat_pvals.1)))
```

# Scrap
## fastLm
```{r}
rand_y <- rnorm(nrow(mtcars))
df <- cbind(y = rand_y, mtcars)

## lm
lm.mdl0.fit <- lm(y ~ mpg + cyl + disp, data = df)
lm.mdl1.fit <- lm(y ~ ., data = df)

## alternatives
X <- cbind("(Intercept)" = 1, as.matrix(mtcars))
X_null <- cbind("(Intercept)" = 1, as.matrix(mtcars)[, c("mpg", "cyl", "disp")])

fastLm.mdl0.fit <- RcppEigen::fastLm(
  X = X_null,
  y = rand_y
)

fastLm.mdl1.fit <- RcppEigen::fastLm(
  X = X,
  y = rand_y
)
```

```{r}
anova_obj <- anova(lm.mdl0.fit, lm.mdl1.fit)
anova_obj$F
anova_obj$`Pr(>F)`

microbenchmark::microbenchmark(
  anova(lm.mdl0.fit, lm.mdl1.fit),
  fastLM_ftest(lm.mdl0.fit, lm.mdl1.fit),
  times = 1000
)

fastLM_ftest(fastLm.mdl0.fit, fastLm.mdl1.fit)
summary(fastLm.mdl0.fit)
mdl1.fit <- fastLm.mdl0.fit
```

## repmat
### tca.fit
```{r}
# tca.fit
# saveRDS(list(a = W_norms[, j], n = k, m = 1), "assets/repmat_optimize.rds")
# stop()
repmat_1 <- readRDS("assets/repmat_optimize.rds")
waldo::compare(
  t(repmat(repmat_1$a, repmat_1$n, repmat_1$m)),
  MESS::repmat(matrix(repmat_1$a), repmat_1$m, repmat_1$n)
)

t(repmat(repmat_1$a, repmat_1$n, repmat_1$m))
MESS::repmat(matrix(repmat_1$a), ncol = repmat_1$n)
waldo::compare(
  pracma::repmat(repmat_1$a, 1000, 1),
  MESS::repmat(matrix(repmat_1$a, nrow = 1), 1000, 1)
)
```

### tca.fit_means_vars
```{r}
# tca.fit_means_vars
# saveRDS(
#   list(
#     W_norms = W_norms,
#     X_tilde = W_tilde,
#     C1_tilde = C1_tilde,
#     C2_tilde = C2_tilde
#   ),
#   "assets/repmat_optimize2.rds"
# )
# stop()
repmat_2 <- readRDS("assets/repmat_optimize2.rds")
waldo::compare(
  apply(repmat_2$X_tilde, 2, function(v) {
    repmat(v, 1, 3)
  }),
  apply(repmat_2$X_tilde, 2, function(v) {
    MESS::repmat(matrix(v), ncol = 3)
  })
)
```

### minus_log_likelihood_sigmas
```{r}
# minus_log_likelihood_sigmas
# saveRDS(
#   list(
#     w_i_rep = w_i_rep,
#     sigmas_squared = sigmas_squared,
#     V_rep = V_rep,
#     mus = mus,
#     C_tilde = C_tilde,
#     U_i = U_i,
#     k = k,
#     V_rep = V_rep,
#     U_i_squared = U_i_squared,
#     V = V
#   ),
#   "assets/repmat_optimize4.rds"
# )
# stop()

repmat_3 <- readRDS("assets/repmat_optimize3.rds")

for (i in names(repmat_3)) {
  assign(i, repmat_3[[i]])
}

gradient <- -(
  colSums(
    W_squared * pracma::repmat(sigmas, n, 1) * t(pracma::repmat(U_j, k, 1)) / pracma::repmat(V_squared, 1, k)
  ) - colSums(
    W_squared * pracma::repmat(sigmas, n, 1) / pracma::repmat(V, 1, k)
  )
)

gradient2 <- -(
  colSums(
    (
      W_squared *
        MESS::repmat(matrix(sigmas, nrow = 1), nrow = n, 1) *
        MESS::repmat(matrix(U_j), ncol = k)
    ) /
      MESS::repmat(V_squared, 1, ncol = k)
  ) -
    colSums(
      (
        W_squared *
          MESS::repmat(matrix(sigmas, nrow = 1), nrow = n, 1)
      ) /
        MESS::repmat(V, 1, ncol = k)
    )
)

W_squared_sig <- W_squared * MESS::repmat(matrix(sigmas, nrow = 1), nrow = n, 1)

gradient3 <- -(
  colSums(
    (
      W_squared_sig *
        MESS::repmat(matrix(U_j), ncol = k)
    ) /
      MESS::repmat(V_squared, 1, ncol = k)
  ) -
    colSums(
      (
        W_squared_sig
      ) /
        MESS::repmat(V, 1, ncol = k)
    )
)

waldo::compare(gradient, gradient2)
waldo::compare(gradient, gradient3)
```

### minus_log_likelihood_w
