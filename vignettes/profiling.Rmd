---
title: "profiling"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{profiling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = FALSE,
  comment = "#>"
)
```

# Synopsis
* `{TCA}` is a great package. But it is running unexpectedly slowly. A 
moderately sized project (400,000 CpGs * 1000 Samples * 6 Cell Types, 
`vars.mle = FALSE`) couldn't converge after 12 hours and hit out of memory error 
with 120GB of RAM.  
* This fork is created to profile the performance of `{TCA}` and see if there are 
"low hanging fruit" bottlenecks we can resolve.  

# Methods
* Use the amazing `TCA::test_data()` function to simulate data under different
scenarios  
* Profile with `profvis::profvis()` and identify bottlenecks  
* Read the source codes and determine if there are low hanging fruits  
* Created replicates of fit results from `{TCA}` version 1.2.1 as test fixtures  
* Make small changes, fit the model, and test if all the fit results are within 
machine tolerance of differences with the test fixtures  
* Profile the modded version under sequential, `vars.mle = TRUE`, `refit_W = TRUE`,
`TCA::tcareg()` fit, and parallel runs  

# Results
* After modifications, running sequentially, we achieved a **6x** speed up in a
simulation (Fig 1).  
* Running in parallel over chunks of X, we achieved a **13.3x** speed up in a 
simulation with `vars.mle = TRUE` (Fig 4).  

![Fig 1, 1.2.1 (left) vs modded (right)](prof_fig_1.2.1_v_mod.png)

</br>  

* The major source of speed up achieved is from replacing the `data.frame()`, 
`lm()`, and `anova()` calls with `RcppEigen::fastLm()` calls. This also helped 
with <span style="background-color: #f8e600">memory usage by 2x</span>. This is 
potentially an unsafe change. But the risk is small because    
  + The constrained optimization step before `fastLm()` has internal checks for 
  matrix positive definiteness. Indeed, testing using rank deficient C1 and C2 
  matrices tripped the matrix positive definite checks before `fastLm()` is even
  called.  
  + `RcppEigen::fastLm()` returns the correct values for rank deficient matrices,
  and we can then use `anyNA()` to check for `NA` of p-values for gamma hats if
  the optimization throw an error albeit being unnecessary.  
  + `NA` is already expected to be taken care of by users.  
  + For 5 repetitions, the returned estimates are the same between the `fastLm()` 
  version and `TCA::tca()` version 1.2.1.  
  + Since it looks like `{TCA}` is only running partial F-tests between 2 models, 
  we don't need the extra information of `lm()` objects or the checks and 
  formatting of `stats::anova()` calls. Implementing a minimal version of 
  the partial F-test helped reduced fit time.  
  + We can also use `lm.fit()` as well to minimize dependencies. `lm.fit()` comes 
  with more safety checks and probably no significant differences in performance
  compared to `fastLm()`. But `{RcppEigen}` is so popular that it is probably 
  already installed on most machines so let's stay with `fastLm()` for now.  
* Replacing `pracma::repmat()` calls with `MESS::repmat()`, which directly calls
C codes, helped a lot as well. This is because the optimization steps repeatedly 
calls on this function. Reduced the `tca.fit_means_vars()` times by **1.6x**.  
* Main bottleneck of `vars.mle = TRUE` fit is from `nloptr()`. A good workflow 
might be to fit with the alternative optimization for model iteration and then 
fit one final fit with `vars.mle = TRUE`. Achieved a **3.3x** improvement mostly
through replacing the 
<span style="background-color: #f8e600">replacing the `repmat()` calls </span>
(Fig 2).   

![Fig 2, vars.mle = TRUE, 1.2.1 (left) vs modded (right)](prof_fig_vars.mle.png)

</br>  

* Importantly, fit results using `{TCA}` version 1.2.1 under multiple repeats and 
scenarios are saved as test fixtures to make sure changes in `./R/model_fit.R` 
gives back the same estimates.  

# Best practices
* We probably shouldn't use the parallel argument in `TCA::tca()`.  
* On a Windows machine, the <span style="background-color: #26d800">overhead</span> caused by data being shuffled back and 
forth by PSOCK increased the fit time by **7x** in a small simulation.  
  
  ![Fig 3, parallel = TRUE, num_cores = 6 in `tca()` (left) vs sequential (right) PSOCK](prof_fig_seq_v_par_win.png)
  
</br>  

* On a Linux machine, the performance using 3 clusters is slower than the 
sequential run by **1.1x** in a simulation. However, on a HPC where small latency 
can add up or where huge data is passed to `TCA::tca()`, the data has to be 
replicated for each fork and the performance might worsen and consume much 
more memory for slower speed.  
* [The correct way](https://github.com/cozygene/TCA/issues/1#issuecomment-524380136) to parallel `TCA::tca()` is to chunk the data by X and run 
parallel over each chunk instead. For example, for 400,000 CpGs, chunk into 
7 x 50,000\*m chunks or 14 x 30,000\*m chunks and parallel over these chunks for 
7 cores on a 8 cores machine for example.  
* However, since the optimization of sigmas_hat and tau_hat uses information 
from X, it is unconfirmed if chunking by X is valid since the optimization 
techinically is only using information provided by each chunk.  
  + Using `vars.mle = TRUE`, the correlation between parameters of a sequential and
  an extreme chunked run where each CpG is in a chunk is at 0.99. With 
  `vars.mle = FALSE`, this drops to 0.88.  
  + This is a favorable trade-off if the author confirm that the findings is 
  correct. In this simulation, we achieved a **13.3x** increase in performance  

![Fig 4, 1.2.1 sequential (top) vs modded parallel by chunk X (bottom) ](1.2.1_seq_v_modded_chunk_X_par_win.png)

* If `refit_W` is needed. Run the most informative sites instead of the whole 
epi-genome.  

# Changelog
[23/03/26]  

* Added wrappers for running tca by chunks of matrix X in `./R/tca_split.R`.

[23/03/25]  

* Moved some function calls outside of `minus_log_likelihood_tau()` and
`minus_log_likelihood_sigmas()` to shave off a couple of seconds through 
possible repeated calls of `nrow()` and `length()` in `nloptr::nloptr()`.  

[23/03/24]  

* Replaced all the `pracma::repmat()` calls with `MESS::repmat()` calls 
outside of the `tcareg()` related functions (because `tcareg()` is fast enough).  
* Added test for no result change from ver 1.2.1 for `vars.mle = TRUE`.  
* Added test for no result change from ver 1.2.1 for `refit_W()`.  

[23/03/23]  

* Replaced `data.frame()` calls and `lm()` calls in `./R/model_fit.R`  
* Added `fastLM_ftest()` to `./R/utils.R` to calculate partial f-tests.  
* Added test for no result change from ver 1.2.1.  


# Profiling
```{r eval = FALSE}
library(profvis) # Profiling tool
library(devtools)
library(tictoc)
near <- dplyr::near
load_all()
```

```{r}
# Simulate the data
set.seed(1234)
library(TCA) # Version 1.2.1
data <- test_data(100, 10000, 6, 1, 1, 0.01)
# tca.mdl <- tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2)
lapply(data, dim)
```

## Windows machine
* Cluster type will be set to PSOCK. This would be the worst case scenario for
TCA since PSOCK has to shuttle data back and forth between the workers and this
massively increases the overhead. In the results below, the fit time was increased
by 7x.  

```{r, eval = FALSE} 
set.seed(1234)
# Run the data sequentially first. This is the results for ver 1.2.1.
prof_obj <- profvis({
  tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2)
})
saveRDS(prof_obj, "./assets/1_2_1_sequential_n100_m1e5.rds")

# 1.2.1 vars.mle = TRUE
prof_obj <- profvis({
  tca(
    X = data$X, W = data$W, C1 = data$C1, C2 = data$C2,
    vars.mle = TRUE
  )
})
saveRDS(prof_obj, "./assets/1_2_1_sequential_vars.mle_n100_m1e5.rds")

# Then remove.packages("TCA") and load_all() this branch. Also run sequentially
remove.packages("TCA")
load_all()
prof_obj <- profvis({
  tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2)
})
saveRDS(prof_obj, "./assets/modded_sequential_n100_m1e5.rds")

# modded vars.mle = TRUE
prof_obj <- profvis({
  tca(
    X = data$X, W = data$W, C1 = data$C1, C2 = data$C2,
    vars.mle = TRUE
  )
})
saveRDS(prof_obj, "./assets/modded_sequential_vars.mle.n100_m1e5.rds")

# Then test the parallel performance
prof_obj <- profvis({
  tca(
    X = data$X, W = data$W, C1 = data$C1, C2 = data$C2, parallel = TRUE,
    num_cores = 6L
  )
})
saveRDS(prof_obj, "./assets/modded_parallel_n100_m1e5.rds")

# Test the parallel performance chunked by X, vars.mle = TRUE
library(furrr)
prof_obj <- profvis({
  split_X <- split_input(X = data$X, n_chunks = 7)
  
  plan(multisession, workers = 7)
  
  res_par <- tca_split(
    X = split_X,
    W = data$W,
    C1 = data$C1,
    C2 = data$C2,
    vars.mle = TRUE,
    max_iters = 20
  )
  
  plan(sequential)
})
saveRDS(prof_obj, "./assets/modded_parallel_chunk_X_vars.mle_n100_m1e5.rds")
```

* The data graph shows 2x memory saving and 6x performance increase.  
```{r, eval = FALSE}
seq_1_2_1 <- readRDS("./assets/1_2_1_sequential_n100_m1e5.rds")
seq_1_2_1.mle <- readRDS("./assets/1_2_1_sequential_vars.mle_n100_m1e5.rds")
seq_modded <- readRDS("./assets/modded_sequential_n100_m1e5.rds")
seq_modded.mle <- readRDS("./assets/modded_sequential_vars.mle.n100_m1e5.rds")
seq_parallel_modded <- readRDS("./assets/modded_parallel_n100_m1e5.rds")
chunk_X_modded.mle <- readRDS("./assets/modded_parallel_chunk_X_vars.mle_n100_m1e5.rds")

# 1.2.1 vs modded
seq_1_2_1
seq_modded

# 1.2.1 vs modded mle
seq_1_2_1.mle
seq_modded.mle

# modded sequential vs parallel
seq_modded
seq_parallel_modded

#  1.2.1 vars mle vs chunk by X modded
seq_1_2_1.mle
chunk_X_modded.mle
```

## Linux machine
* The forked cluster is available on linux machines.  
```{r}
set.seed(1234)
prof_obj <- profvis({
  tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2)
})
saveRDS(prof_obj, "./assets/linux_modded_sequential_n100_m1e5.rds")

prof_obj <- profvis({
  tca(
    X = data$X, W = data$W, C1 = data$C1, C2 = data$C2, parallel = TRUE,
    num_cores = 3L
  )
})
saveRDS(prof_obj, "./assets/linux_modded_parallel_n100_m1e5.rds")
```

* Slower by 1.08x.  
```{r}
seq_fork_modded <- readRDS("./assets/linux_modded_sequential_n100_m1e5.rds")
seq_fork_modded_par <- readRDS("./assets/linux_modded_parallel_n100_m1e5.rds")
seq_fork_modded
seq_fork_modded_par
```

# Notes
* We see that in `model_fit.R/tca_fit.R`, R is spending  
  + High peak memory and decent amount of time to call `tca.fit_means_vars()`. 
  This is the internal loop optimization of `tca()`. This is probably where the 
  bottle neck for the loop in parallel mode because the high memory overhead.  
  + High peak memory and a lot of time to call `lm()` and subsequently 
  `model.frame.default()`.  
  + The `anova.lm()` call to perform partial F tests is also taking a 
  significant amount time.   
* Let's try to replace the `data.frame()` call with a straight `cbind()` call to 
create a X matrix. Then we can use `RcppEigen::fastLm()` to directly call 
the X data matrix and the vector y.  
  + This is potentially a dangerous trade-off between speed and safety. But 
  `NA` is already expected to be taken care of by user. `{quadprog}` has 
  internal check for positive definite matrix. Furthermore, `RcppEigen::fastLm()` 
  returns the correct results for rank deficient matrix. We should be safe to 
  make this trade off. We can create a test case for this.  
  + Have to implement an `anova()` method for `RcppEigen::fastLm()`. This is 
  straightforward.  
* The `pracma::repmat()` calls are also taking a good amount of time. Looks like
the implementation in `{MESS}` accomplish the same thing but calls C code 
directly so it should be faster. This change would add up since `pracma::repmat()` 
is called a lot.

# Replacing codes
* Warning, this might be unsafe  

## fastLm
* Replace the `data.frame()` and `lm()` call with `RcppEigen::fastLm`  
* Two code chunks to be replaced.  
```{r, eval = F}
# Before
df <-
  data.frame(y = X_tilde[, j], cbind(
    W / t(repmat(W_norms[, j], k, 1)),
    if (p2 > 0) {
      C2 / t(repmat(W_norms[, j], p2, 1))
    } else {
      C2
    },
    if (p1 > 0) {
      C1_ / t(repmat(W_norms[, j], k * p1, 1))
    } else {
      C1_
    }
  ))
mdl1.fit <- lm(y ~ ., data = df)
mdl1.coef <- summary(mdl1.fit)$coefficients
mdl1.cov.names <- colnames(df)[colnames(df) != "y"]
deltas_gammas_hat_pvals <-
  sapply(mdl1.cov.names, function(x) {
    if (x %in% rownames(mdl1.coef)) {
      return(mdl1.coef[x, "Pr(>|t|)"])
    } else {
      return(NA)
    }
  })
```

```{r, eval = F}
# After
mdl1.fit <- RcppEigen::fastLm(
  X = cbind(
    "(Intercept)" = 1.0, # <----------- Remember the intercept
    W / t(repmat(W_norms[, j], k, 1)),
    if (p2 > 0) {
      C2 / t(repmat(W_norms[, j], p2, 1))
    } else {
      C2
    },
    if (p1 > 0) {
      C1_ / t(repmat(W_norms[, j], k * p1, 1))
    } else {
      C1_
    }
  ),
  y = X_tilde[, j]
)
mdl1.coef <- summary(mdl1.fit)$coefficients
# First row is always intercept. Sacrifice some code readability here
# Sacrifice some code readability here by using -1 instead of
## `which(rownames(mdl1.coef) != "(Intercept)")`
deltas_gammas_hat_pvals <- mdl1.coef[-1, "Pr(>|t|)"]
```

Second chunk  
```{r, eval = F}
# Before
C1_alt <- C1_ / t(repmat(W_norms[, j], k * p1, 1))
for (d in 1:p1) {
  C1_null <- C1_alt[, setdiff(1:(p1 * k), seq(d, k * p1, p1))]
  df <-
    data.frame(y = X_tilde[, j], cbind(W / t(repmat(W_norms[, j], k, 1)), if (p2 > 0) {
      C2 / t(repmat(W_norms[, j], p2, 1))
    } else {
      C2
    }, C1_null))
  mdl0.fit <- lm(y ~ ., data = df)
  anova.fit <- anova(mdl0.fit, mdl1.fit)
  gammas_hat_pvals.joint[d] <- anova.fit$`Pr(>F)`[2]
}
```

```{r, eval = F}
# After
for (d in 1:p1) {
  mdl0.fit <- RcppEigen::fastLm(
    X = cbind(
      "(Intercept)" = 1.0, # <----------- Remember the intercept
      W / t(repmat(W_norms[, j], k, 1)),
      if (p2 > 0) {
        C2 / t(repmat(W_norms[, j], p2, 1))
      } else {
        C2
      },
      #### Used to be `C1_null` and `C1_alt`. Removed assignment calls.
      (C1_ / t(repmat(W_norms[, j], k * p1, 1)))[, setdiff(1:(p1 * k), seq(d, k * p1, p1))]
    ),
    y = X_tilde[, j]
  )
  gammas_hat_pvals.joint[d] <- fastLM_ftest(mdl0.fit, mdl1.fit)$`Pr(>F)`
}
```

## repmat
* `pracma::repmat()` is taking a decent chunk out of the time. 
`MESS::repmat` is about 50% faster at the current dimensions.  
```{r}
use_package("MESS")
```

* Replace a bunch of `t(repmat ...)` with `MESS::repmat` to remove a `t()` call 
and use the more efficient `MESS::repmat()` call.  

* For `vars.mle = TRUE`, carefully reproduce the gradient calculation and replace
with `MESS::repmat()` as well as removing some repeated calculations.  

### minus_log_likelihood_sigmas
```{r, eval = FALSE}
# Before
return(list(
  "objective" = -0.5 * (const - sum(log(V)) - sum(U_j / V)),
  "gradient" = -(colSums(W_squared * repmat(sigmas, n, 1) * t(repmat(U_j, k, 1)) /
    repmat(V_squared, 1, k)) - colSums(W_squared * repmat(sigmas, n, 1) / repmat(V, 1, k)))
))
```

```{r}
# After
W_squared_sig <- W_squared * MESS::repmat(matrix(sigmas, nrow = 1), nrow = n, 1)
return(list(
  "objective" = -0.5 * (const - sum(log(V)) - sum(U_j / V)),
  "gradient" = -(
    colSums(W_squared_sig * MESS::repmat(matrix(U_j), ncol = k) /
      MESS::repmat(V_squared, 1, ncol = k)) -
      colSums(W_squared_sig / MESS::repmat(V, 1, ncol = k))
  )
))
```

### minus_log_likelihood_w
```{r}
# Before
V_rep <- repmat(V, 1, k)
U_i <- tcrossprod(mus, w_i) + crossprod_deltas_c2_i + tcrossprod(gammas, c1_i_) - t(x_i)
U_i_squared <- U_i**2
w_i_rep <- repmat(w_i, m, 1)
fval <- -0.5 * (const - sum(log(V)) - sum(U_i_squared / V))
gval <- colSums(w_i_rep * sigmas_squared / V_rep) + colSums(((mus + C_tilde) * repmat(U_i, 1, k) * V_rep - w_i_rep * sigmas_squared * repmat(U_i_squared, 1, k)) / repmat(V**2, 1, k))
return(list("objective" = fval, "gradient" = gval))
```

```{r}
# After
V_rep <- MESS::repmat(V, 1, k)
U_i <- tcrossprod(mus, w_i) + crossprod_deltas_c2_i + tcrossprod(gammas, c1_i_) - t(x_i)
U_i_squared <- U_i**2
w_i_rep <- MESS::repmat(matrix(w_i, nrow = 1), m, 1)
fval <- -0.5 * (const - sum(log(V)) - sum(U_i_squared / V))
w_i_rep_sig <- w_i_rep * sigmas_squared
gval <-
  colSums(w_i_rep_sig / V_rep) +
  colSums((
    (mus + C_tilde) * MESS::repmat(U_i, 1, k) * V_rep -
      w_i_rep_sig * MESS::repmat(U_i_squared, 1, k)
  ) /
    MESS::repmat(V**2, 1, k))
return(list("objective" = fval, "gradient" = gval))
```

## vars.mle = TRUE
* Not really a way to get around the `nloptr()` call that takes the majority of the
time to optimize sigma.  
* The only "improvement" that's low hanging is remove assignment calls for objects
that is used only once in the function to minimize overhead for `nloptr()`

## parallel
* Let's think about the parallel.  
* Looks like R is stopping and starting clusters multiple times.  
  + Cluster is started twice. Once for the `tca.fit_mean_vars()` and once for 
  p-values of deltas and gammas.  
  + RUNNING CLUSTER IS IN GENERALL MUCH SLOWER THAN SEQUENTIAL! This is probably
  because of overhead.
  + Make sure to only run in sequential mode. But parallel over chunks of 
  X matrix instead.  
* Add a stop for if parallel and refit_W is FALSE. We can probably remove all
the parallel to be honest and just run the codes over chunks of X.  

## removing memory consuming `<-` calls
* (TO DO) Check for memory consuming `<-` calls  

# Test
## Did results change?
```{r}
data <- readRDS(test_path("fixtures", "sim1_simdata.rds"))[1, ]
fitted <- readRDS(test_path("fixtures", "sim1_exp_2_fit_1_2_1.rds"))[1, ]

set.seed(1234)

fitted$new_fit <- lapply(
  data$df,
  \(d) {
    tca(
      X = d$X,
      W = d$W,
      C1 = d$C1,
      C2 = d$C2,
      vars.mle = TRUE
    )
  }
)

sapply(
  1:length(fitted$fit_1_2_1[[1]]),
  \(x) {
    max(fitted$fit_1_2_1[[1]][[x]] - fitted$new_fit[[1]][[x]])
  }
)

fitted$fit_1_2_1[[1]] - fitted$new_fit[[1]]
compare_fit <- function(o, n) {
  all(sapply(names(o), \(x) {
    all(near(o[[x]], n[[x]]))
  }))
}

fitted$results <- purrr::map2_lgl(
  fitted$fit_1_2_1,
  fitted$new_fit,
  compare_fit
)

plan(sequential)
# waldo::compare(fitted$deltas_hat_pvals, tca.mdl$deltas_hat_pvals)
```

## fastLm
The two code chunks that were replaced has to be tested concurrently  
```{r}
data <- test_data(30, 1000, 6, 1, 1, 0.01)
C1_1 <- cbind(data$C1, data$C1)
C2_1 <- cbind(data$C2, data$C2)
tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2)
df <- readRDS("./assets/change_1.rds")
X <- cbind("(Intercept)" = 1, df[, which(names(df) != "y")])
y <- df$y

mdl1.fit <- lm(y ~ ., data = df)
mdl1.coef <- summary(mdl1.fit)$coefficients
mdl1.cov.names <- colnames(df)[colnames(df) != "y"]
deltas_gammas_hat_pvals <-
  sapply(mdl1.cov.names, function(x) {
    if (x %in% rownames(mdl1.coef)) {
      return(mdl1.coef[x, "Pr(>|t|)"])
    } else {
      return(NA)
    }
  })

deltas_gammas_hat_pvals

mdl1.fit.1 <- RcppEigen::fastLm(
  X = X,
  y = y
)

mdl1.coef.1 <- summary(mdl1.fit.1)$coefficients
deltas_gammas_hat_pvals.1 <- mdl1.coef.1[-1, "Pr(>|t|)"]
stopifnot(all(dplyr::near(deltas_gammas_hat_pvals, deltas_gammas_hat_pvals.1)))
```

# Scrap
```{r}
library(profvis)
set.seed(1234)
data <- test_data(1000, 2000, 12, 3, 10, 0.01)

# Sequential
sim_1 <- profvis({
  tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2)
})

# Parallel
sim_2 <- profvis({
  tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2, parallel = TRUE, num_cores = 4L)
})
```

## fastLm
```{r}
rand_y <- rnorm(nrow(mtcars))
df <- cbind(y = rand_y, mtcars)

## lm
lm.mdl0.fit <- lm(y ~ mpg + cyl + disp, data = df)
lm.mdl1.fit <- lm(y ~ ., data = df)

## alternatives
X <- cbind("(Intercept)" = 1, as.matrix(mtcars))
X_null <- cbind("(Intercept)" = 1, as.matrix(mtcars)[, c("mpg", "cyl", "disp")])

fastLm.mdl0.fit <- RcppEigen::fastLm(
  X = X_null,
  y = rand_y
)

fastLm.mdl1.fit <- RcppEigen::fastLm(
  X = X,
  y = rand_y
)
```

```{r}
anova_obj <- anova(lm.mdl0.fit, lm.mdl1.fit)
anova_obj$F
anova_obj$`Pr(>F)`

microbenchmark::microbenchmark(
  anova(lm.mdl0.fit, lm.mdl1.fit),
  fastLM_ftest(lm.mdl0.fit, lm.mdl1.fit),
  times = 1000
)

fastLM_ftest(fastLm.mdl0.fit, fastLm.mdl1.fit)
summary(fastLm.mdl0.fit)
mdl1.fit <- fastLm.mdl0.fit
```

### Bug
```{r}
set.seed(1234)
data <- test_data(500, 5000, 6, 1, 1, 0.01)
tca.mdl <-
  tca(
    X = data$X,
    W = data$W,
    C1 = data$C1,
    C2 = data$C2
  )
```

### Positive Definite
```{r}
X <- matrix(rnorm(100), ncol = 4)
X <- cbind(X, X[, 1], X[, 2])
X
y <- rnorm(nrow(X))

df <- as.data.frame(cbind(X, y))
lm(y ~ ., data = df) |>
  summary()

library(RcppEigen)
mdl1.fit <- fastLm(X = X, y = y)

mdl1.coef <- summary(mdl1.fit)$coefficients
# First row is always intercept. Sacrifice some code readability here
# Sacrifice some code readability here by using -1 instead of
## mdl1.coef[`which(rownames(mdl1.coef) != "(Intercept)")`, "Pr(>|t|)"]
deltas_gammas_hat_pvals <- mdl1.coef[-1, "Pr(>|t|)"]
stopifnot("C1 is rank deficient" = !anyNA(deltas_gammas_hat_pvals))
```

## repmat
### tca.fit
```{r}
# tca.fit
# saveRDS(list(a = W_norms[, j], n = k, m = 1), "assets/repmat_optimize.rds")
# stop()
repmat_1 <- readRDS("assets/repmat_optimize.rds")
waldo::compare(
  t(repmat(repmat_1$a, repmat_1$n, repmat_1$m)),
  MESS::repmat(matrix(repmat_1$a), repmat_1$m, repmat_1$n)
)

t(repmat(repmat_1$a, repmat_1$n, repmat_1$m))
MESS::repmat(matrix(repmat_1$a), ncol = repmat_1$n)
waldo::compare(
  pracma::repmat(repmat_1$a, 1000, 1),
  MESS::repmat(matrix(repmat_1$a, nrow = 1), 1000, 1)
)
```

### tca.fit_means_vars
```{r}
# tca.fit_means_vars
# saveRDS(
#   list(
#     W_norms = W_norms,
#     X_tilde = W_tilde,
#     C1_tilde = C1_tilde,
#     C2_tilde = C2_tilde
#   ),
#   "assets/repmat_optimize2.rds"
# )
# stop()
repmat_2 <- readRDS("assets/repmat_optimize2.rds")
waldo::compare(
  apply(repmat_2$X_tilde, 2, function(v) {
    repmat(v, 1, 3)
  }),
  apply(repmat_2$X_tilde, 2, function(v) {
    MESS::repmat(matrix(v), ncol = 3)
  })
)
```

### minus_log_likelihood_sigmas
```{r}
# minus_log_likelihood_sigmas
# stop()

repmat_3 <- readRDS("assets/repmat_optimize3.rds")

for (i in names(repmat_3)) {
  assign(i, repmat_3[[i]])
}

gradient <- -(
  colSums(
    W_squared * pracma::repmat(sigmas, n, 1) * t(pracma::repmat(U_j, k, 1)) / pracma::repmat(V_squared, 1, k)
  ) - colSums(
    W_squared * pracma::repmat(sigmas, n, 1) / pracma::repmat(V, 1, k)
  )
)

gradient2 <- -(
  colSums(
    (
      W_squared *
        MESS::repmat(matrix(sigmas, nrow = 1), nrow = n, 1) *
        MESS::repmat(matrix(U_j), ncol = k)
    ) /
      MESS::repmat(V_squared, 1, ncol = k)
  ) -
    colSums(
      (
        W_squared *
          MESS::repmat(matrix(sigmas, nrow = 1), nrow = n, 1)
      ) /
        MESS::repmat(V, 1, ncol = k)
    )
)

W_squared_sig <- W_squared * MESS::repmat(matrix(sigmas, nrow = 1), nrow = n, 1)

gradient3 <- -(
  colSums(
    (
      W_squared_sig *
        MESS::repmat(matrix(U_j), ncol = k)
    ) /
      MESS::repmat(V_squared, 1, ncol = k)
  ) -
    colSums(
      (
        W_squared_sig
      ) /
        MESS::repmat(V, 1, ncol = k)
    )
)

waldo::compare(gradient, gradient2)
waldo::compare(gradient, gradient3)
```

### minus_log_likelihood_w
```{r}
set.seed(1234)
data <- test_data(20, 10000, 6, 1, 1, 0.01)
tca.mdl <- tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2, refit_W = TRUE)
```

```{r}
# saveRDS(
#   list(
#     w_i = w_i,
#     w_i_rep = w_i_rep,
#     sigmas_squared = sigmas_squared,
#     V_rep = V_rep,
#     mus = mus,
#     C_tilde = C_tilde,
#     U_i = U_i,
#     k = k,
#     V_rep = V_rep,
#     U_i_squared = U_i_squared,
#     V = V
#   ),
#   "assets/repmat_optimize4.rds"
# )
# stop()

repmat_4 <- readRDS("assets/repmat_optimize4.rds")

for (i in names(repmat_4)) {
  assign(i, repmat_4[[i]])
}

V_rep2 <- MESS::repmat(V, 1, k)
V_rep <- repmat(V, 1, k)
waldo::compare(V_rep2, V_rep)

w_i_rep2 <- MESS::repmat(matrix(w_i, nrow = 1), m, 1)
w_i_rep <- repmat(w_i, m, 1)
waldo::compare(w_i_rep2, w_i_rep)

gval <-
  colSums(w_i_rep * sigmas_squared / V_rep) +
  colSums(
    (
      (mus + C_tilde) * repmat(U_i, 1, k) * V_rep -
        w_i_rep * sigmas_squared * repmat(U_i_squared, 1, k)
    ) /
      repmat(V**2, 1, k)
  )

w_i_rep_sig <- w_i_rep * sigmas_squared
gval2 <-
  colSums(w_i_rep_sig / V_rep) +
  colSums((
    (mus + C_tilde) * MESS::repmat(U_i, 1, k) * V_rep -
      w_i_rep_sig * MESS::repmat(U_i_squared, 1, k)
  ) /
    MESS::repmat(V**2, 1, k))

waldo::compare(gval, gval2)
```

## vars.mle = TRUE
* Not really a way to get around the `nloptr()` call that takes the majority of the
time to optimize sigma.  
* The only "improvement" that's low hanging is remove extra assignment calls
in the function to minimize overhead for `nloptr()`

```{r}
set.seed(1234)
data <- test_data(200, 5000, 6, 1, 1, 0.01)
tca.mdl <-
  tca(
    X = data$X,
    W = data$W,
    C1 = data$C1,
    C2 = data$C2
  )

sequential_vars.alt <- profvis({
  tca.mdl <-
    tca(
      X = data$X,
      W = data$W,
      C1 = data$C1,
      C2 = data$C2
    )
})

sequential_vars.mle <- profvis({
  tca.mdl <-
    tca(
      X = data$X,
      W = data$W,
      C1 = data$C1,
      C2 = data$C2,
      vars.mle = TRUE
    )
})

sequential_vars.mle2 <- profvis({
  tca.mdl <-
    tca(
      X = data$X,
      W = data$W,
      C1 = data$C1,
      C2 = data$C2,
      vars.mle = TRUE
    )
})

saveRDS(sequential_vars.mle, "./assets/sequential_n200_m1e3_vars.mle.rds")
```

```{r}
sequential_vars.mle <- readRDS("./assets/sequential_n200_m1e3_vars.mle.rds")
sequential_vars.mle
```

### Rfast::colsums
* Rfast::colsums can be faster than colSums. Use with caution. We can try to 
benchmark the fit of the results before and after the use of colsums for a 
close to realistic set of data.  
* Rfast is faster, but it doesn't reproduce the result of fit 1.2.1.  

```{r}
tm <- matrix(rnorm(25e6), nrow = 5000)

waldo::compare(
  colSums(tm),
  Rfast::colsums(tm),
  tolerance = .Machine$double.eps^0.5
)

microbenchmark::microbenchmark(
  colSums(tm),
  # Rfast::colsums(tm),
  matrixStats::colSums2(tm)
)
```

## split_input 
```{r}
data <- test_data(30, 200000, 6, 1, 1, 0.01)
split_X <- split_input(X = data$X, n_chunks = 21)
```
 
```{r}
fit1 <- tca(X = split_X[[1]], W = data$W, C1 = data$C1, C2 = data$C2, W_C1_C2 = FALSE)
fit2 <- tca(X = split_X[[2]], W = data$W, C1 = data$C1, C2 = data$C2, W_C1_C2 = FALSE)

fit_final <- purrr::map2(fit1, fit2, \(x, y) {
  rbind(x, y)
})
fit_final$mus_hat
```

## tca_split
* Split the work into small chunks and test vs running sequentially and see 
if the estimates are similar.  
* Looks like the smaller the chunks, the more the correlation between the 
sequential run and the chunked run decreases. This is probably because the 
sigmas_hat and tau_hat are estimated using information from X. More work to be 
determined to see if its a trade off between time and accuracy if we just chunk
into "small enough" chunks instead.  
* If vars.mle = TRUE, the correlation stays high (>0.95) even when chunked into 
single CpGs/chunk.  

```{r}
library(TCA)
library(furrr)
# devtools::install_github("hhp94/TCA@profiling")

set.seed(1234)
data <- test_data(30, 10000, 6, 1, 1, 0.01)
split_X <- split_input(X = data$X, n_chunks = 10000)
split_X
dim(split_X)

plan(multisession, workers = 7)

res_par <- tca_split(
  X = split_X, W = data$W, C1 = data$C1, C2 = data$C2, vars.mle = TRUE,
  max_iters = 20
)

plan(sequential)

res_seq <- tca(X = data$X, W = data$W, C1 = data$C1, C2 = data$C2, vars.mle = TRUE, max_iters = 20)
res_seq |> lapply(dim)
res_seq$tau_hat
res_par$tau_hat |> mean()

setdiff(names(res_par), "tau_hat")
lapply(setdiff(names(res_par), "tau_hat"), \(x){
  sapply(
    seq_len(ncol(res_par[[x]])),
    \(y) {
      cor(res_par[[x]][, y], res_seq[[x]][, y])
    }
  )
}) |>
  purrr::set_names(setdiff(names(res_par), "tau_hat"))
```


 
